* 
* ==> Audit <==
* |--------------|--------------------------------|----------|------------|---------|---------------------|---------------------|
|   Command    |              Args              | Profile  |    User    | Version |     Start Time      |      End Time       |
|--------------|--------------------------------|----------|------------|---------|---------------------|---------------------|
| service      | backend-srv -n dicoding-ns     | minikube | XXXX\kyogi | v1.31.1 | 12 Aug 23 19:35 +07 |                     |
| service      | karsauisrv -n dicoding-ns      | minikube | XXXX\kyogi | v1.31.1 | 12 Aug 23 19:41 +07 |                     |
| service      | karsauisrv -n dicoding-ns      | minikube | XXXX\kyogi | v1.31.1 | 12 Aug 23 19:43 +07 |                     |
| stop         |                                | minikube | XXXX\kyogi | v1.31.1 | 12 Aug 23 19:44 +07 | 12 Aug 23 19:44 +07 |
| docker-env   | --shell cmd                    | minikube | XXXX\kyogi | v1.31.1 | 13 Aug 23 14:42 +07 |                     |
| update-check |                                | minikube | XXXX\kyogi | v1.31.1 | 13 Aug 23 20:29 +07 | 13 Aug 23 20:29 +07 |
| update-check |                                | minikube | XXXX\kyogi | v1.31.1 | 13 Aug 23 21:03 +07 | 13 Aug 23 21:03 +07 |
| update-check |                                | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 01:38 +07 | 14 Aug 23 01:38 +07 |
| start        |                                | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 14:51 +07 | 14 Aug 23 14:52 +07 |
| service      | list                           | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 15:04 +07 | 14 Aug 23 15:04 +07 |
| service      | prometheus-server -n           | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 15:04 +07 | 14 Aug 23 15:41 +07 |
|              | monitoring                     |          |            |         |                     |                     |
| service      | prometheus-server -n           | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 15:41 +07 | 14 Aug 23 16:17 +07 |
|              | monitoring                     |          |            |         |                     |                     |
| service      | prometheus-server -n           | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 16:17 +07 | 14 Aug 23 16:17 +07 |
|              | monitoring                     |          |            |         |                     |                     |
| service      | prometheus-server -n           | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 16:17 +07 |                     |
|              | monitoring                     |          |            |         |                     |                     |
| service      | grafana -n monitoring          | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 16:18 +07 | 14 Aug 23 16:19 +07 |
| service      | grafana -n monitoring          | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 16:21 +07 |                     |
| service      | list                           | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 16:30 +07 | 14 Aug 23 16:30 +07 |
| service      | list backend-serv              | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 16:31 +07 |                     |
| service      | list backend-srv -n dicoding   | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 16:31 +07 | 14 Aug 23 16:31 +07 |
| service      | backend-srv -n dicoding        | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 16:31 +07 |                     |
| service      | backend-srv -n dicoding-ns     | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 16:31 +07 | 14 Aug 23 16:43 +07 |
| service      | karsauisrv -n dicoding-ns      | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 16:37 +07 | 14 Aug 23 16:40 +07 |
| service      | karsauisrv -n dicoding-ns      | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 16:41 +07 | 14 Aug 23 16:45 +07 |
| service      | list backend-srv -n            | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 16:44 +07 |                     |
|              | dicodingclear                  |          |            |         |                     |                     |
| stop         |                                | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 16:46 +07 | 14 Aug 23 16:48 +07 |
| start        |                                | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 18:20 +07 | 14 Aug 23 18:27 +07 |
| service      | prometheus-server -n           | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 18:30 +07 |                     |
|              | monitoring                     |          |            |         |                     |                     |
| service      | grafana -n monitoring          | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 18:33 +07 |                     |
| service      | backend-srv -n dicoding-ns     | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 18:56 +07 |                     |
| service      | karsauisrv -n dicoding-ns      | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 19:00 +07 | 14 Aug 23 19:01 +07 |
| service      | karsauisrv -n dicoding-ns      | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 19:03 +07 | 14 Aug 23 19:09 +07 |
| service      | karsauisrv -n dicoding-ns      | minikube | XXXX\kyogi | v1.31.1 | 14 Aug 23 19:14 +07 |                     |
| update-check |                                | minikube | XXXX\kyogi | v1.31.1 | 15 Aug 23 00:48 +07 | 15 Aug 23 00:48 +07 |
| update-check |                                | minikube | XXXX\kyogi | v1.31.1 | 15 Aug 23 00:50 +07 | 15 Aug 23 00:50 +07 |
| update-check |                                | minikube | XXXX\kyogi | v1.31.1 | 15 Aug 23 11:28 +07 | 15 Aug 23 11:28 +07 |
| update-check |                                | minikube | XXXX\kyogi | v1.31.1 | 15 Aug 23 11:31 +07 | 15 Aug 23 11:31 +07 |
| update-check |                                | minikube | XXXX\kyogi | v1.31.1 | 15 Aug 23 23:05 +07 | 15 Aug 23 23:05 +07 |
| update-check |                                | minikube | XXXX\kyogi | v1.31.1 | 16 Aug 23 01:31 +07 | 16 Aug 23 01:31 +07 |
| start        |                                | minikube | XXXX\kyogi | v1.31.1 | 23 Aug 23 18:16 +07 | 23 Aug 23 18:17 +07 |
| service      | list                           | minikube | XXXX\kyogi | v1.31.1 | 23 Aug 23 18:27 +07 | 23 Aug 23 18:27 +07 |
| stop         |                                | minikube | XXXX\kyogi | v1.31.1 | 23 Aug 23 18:47 +07 | 23 Aug 23 18:48 +07 |
| delete       |                                | minikube | XXXX\kyogi | v1.31.1 | 23 Aug 23 18:49 +07 | 23 Aug 23 18:49 +07 |
| start        |                                | minikube | XXXX\kyogi | v1.31.1 | 24 Aug 23 15:09 +07 | 24 Aug 23 15:10 +07 |
| delete       |                                | minikube | XXXX\kyogi | v1.31.1 | 24 Aug 23 16:46 +07 | 24 Aug 23 16:50 +07 |
| start        |                                | minikube | XXXX\kyogi | v1.31.1 | 24 Aug 23 16:57 +07 | 24 Aug 23 16:59 +07 |
| tunnel       |                                | minikube | XXXX\kyogi | v1.31.1 | 24 Aug 23 17:08 +07 |                     |
| delete       |                                | minikube | XXXX\kyogi | v1.31.1 | 24 Aug 23 17:18 +07 | 24 Aug 23 17:18 +07 |
| delete       |                                | minikube | XXXX\kyogi | v1.31.1 | 24 Aug 23 17:25 +07 | 24 Aug 23 17:25 +07 |
| docker-env   | --shell cmd                    | minikube | XXXX\kyogi | v1.31.1 | 26 Aug 23 17:19 +07 |                     |
| docker-env   | --shell cmd                    | minikube | XXXX\kyogi | v1.31.1 | 26 Aug 23 17:42 +07 |                     |
| delete       |                                | minikube | XXXX\kyogi | v1.31.1 | 26 Aug 23 18:00 +07 | 26 Aug 23 18:00 +07 |
| config       | set memory 4000                | minikube | XXXX\kyogi | v1.31.1 | 26 Aug 23 18:01 +07 | 26 Aug 23 18:01 +07 |
| start        |                                | minikube | XXXX\kyogi | v1.31.1 | 26 Aug 23 18:01 +07 |                     |
| config       | set memory 3000                | minikube | XXXX\kyogi | v1.31.1 | 26 Aug 23 18:02 +07 | 26 Aug 23 18:02 +07 |
| start        |                                | minikube | XXXX\kyogi | v1.31.1 | 26 Aug 23 18:02 +07 | 26 Aug 23 18:03 +07 |
| tunnel       |                                | minikube | XXXX\kyogi | v1.31.1 | 26 Aug 23 18:14 +07 | 26 Aug 23 18:15 +07 |
| delete       |                                | minikube | XXXX\kyogi | v1.31.1 | 26 Aug 23 18:19 +07 | 26 Aug 23 18:19 +07 |
| config       |                                | minikube | XXXX\kyogi | v1.31.1 | 12 Sep 23 09:06 +07 | 12 Sep 23 09:06 +07 |
| stop         |                                | minikube | XXXX\kyogi | v1.31.1 | 28 Sep 23 12:20 +07 |                     |
| start        |                                | minikube | XXXX\kyogi | v1.31.1 | 28 Sep 23 13:40 +07 |                     |
|--------------|--------------------------------|----------|------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/09/28 13:40:31
Running on machine: xxxx
Binary: Built with gc go1.20.6 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0928 13:40:31.120399    6040 out.go:296] Setting OutFile to fd 104 ...
I0928 13:40:31.120399    6040 out.go:343] TERM=,COLORTERM=, which probably does not support color
I0928 13:40:31.120399    6040 out.go:309] Setting ErrFile to fd 108...
I0928 13:40:31.120399    6040 out.go:343] TERM=,COLORTERM=, which probably does not support color
I0928 13:40:31.153327    6040 out.go:303] Setting JSON to false
I0928 13:40:31.160384    6040 start.go:128] hostinfo: {"hostname":"xxxx","uptime":112227,"bootTime":1695771003,"procs":284,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.22621.2361 Build 22621.2361","kernelVersion":"10.0.22621.2361 Build 22621.2361","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"33a5c68f-1b0c-43b2-9972-62c7d824995c"}
W0928 13:40:31.160930    6040 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0928 13:40:31.163860    6040 out.go:177] * minikube v1.31.1 on Microsoft Windows 11 Pro 10.0.22621.2361 Build 22621.2361
I0928 13:40:31.167651    6040 notify.go:220] Checking for updates...
I0928 13:40:31.173330    6040 driver.go:373] Setting default libvirt URI to qemu:///system
I0928 13:40:31.175370    6040 global.go:111] Querying for installed drivers using PATH=C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Microservicetools;C:\Java\apache-maven-3.9.2\bin;C:\Program Files\Amazon\AWSCLIV2\;C:\Program Files\Go\bin;C:\Program Files\Git\cmd;C:\ProgramData\chocoportable\bin;C:\Microservicetools\istio-1.17.1\bin;C:\Java\zulu17.44.15-ca-jdk17.0.8-win_x64\bin;C:\Java\nodejs\node_modules\npm\bin;;C:\Program Files\Docker\Docker\resources\bin;C:\Users\kyogi\AppData\Local\Microsoft\WindowsApps;C:\Users\kyogi\AppData\Local\JetBrains\Toolbox\scripts;C:\Users\kyogi\AppData\Local\Programs\Microsoft VS Code\bin;C:\Java\apache-maven-3.9.2\bin;C:\Microservicetools;C:\Program Files\Amazon\AWSCLIV2\;C:\Program Files\Docker\Docker\resources\bin;C:\Program Files\Git\cmd;C:\Java\apache-maven-3.9.2\bin;C:\Users\kyogi\go\bin;C:\Users\kyogi\AppData\Local\GitHubDesktop\bin;C:\Microservicetools\istio-1.17.1\bin;C:\Java\zulu17.44.15-ca-jdk17.0.8-win_x64\bin;C:\Java\nodejs\node_modules\npm\bin;
I0928 13:40:31.175878    6040 global.go:122] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0928 13:40:31.290776    6040 global.go:122] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0928 13:40:31.332372    6040 global.go:122] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in %!P(MISSING)ATH%!R(MISSING)eason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0928 13:40:31.571389    6040 lock.go:35] WriteFile acquiring C:\Users\kyogi\.minikube\last_update_check: {Name:mkc0403022a12b631d980407371fc9871d679fb7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0928 13:40:31.575479    6040 out.go:177] * minikube 1.31.2 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.31.2
I0928 13:40:31.577746    6040 out.go:177] * To disable this notice, run: 'minikube config set WantUpdateNotification false'

I0928 13:40:32.088961    6040 docker.go:121] docker version: linux-24.0.6:Docker Desktop 4.23.0 (120376)
I0928 13:40:32.123143    6040 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0928 13:40:39.277085    6040 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (7.1539419s)
I0928 13:40:39.277834    6040 info.go:266] docker info: {ID:c6ecda50-44f3-4dd0-9dfa-13d2396a72bd Containers:42 ContainersRunning:37 ContainersPaused:0 ContainersStopped:5 Images:25 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:190 OomKillDisable:true NGoroutines:195 SystemTime:2023-09-28 06:40:39.195399974 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:20 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:3962327040 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.21.0-desktop.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.7] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.24.1]] Warnings:<nil>}}
I0928 13:40:39.278341    6040 global.go:122] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0928 13:40:43.157358    6040 global.go:122] hyperv default: true priority: 8, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0928 13:40:43.188600    6040 global.go:122] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in %!P(MISSING)ATH%!R(MISSING)eason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0928 13:40:43.221129    6040 global.go:122] qemu2 default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in %!P(MISSING)ATH%!R(MISSING)eason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0928 13:40:43.221129    6040 driver.go:308] not recommending "ssh" due to default: false
I0928 13:40:43.221129    6040 driver.go:343] Picked: docker
I0928 13:40:43.221129    6040 driver.go:344] Alternatives: [hyperv ssh]
I0928 13:40:43.221129    6040 driver.go:345] Rejects: [virtualbox vmware podman qemu2]
I0928 13:40:43.222186    6040 out.go:177] * Automatically selected the docker driver. Other choices: hyperv, ssh
I0928 13:40:43.223241    6040 start.go:298] selected driver: docker
I0928 13:40:43.223241    6040 start.go:898] validating driver "docker" against <nil>
I0928 13:40:43.223241    6040 start.go:909] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0928 13:40:43.256154    6040 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0928 13:40:44.207686    6040 info.go:266] docker info: {ID:c6ecda50-44f3-4dd0-9dfa-13d2396a72bd Containers:42 ContainersRunning:37 ContainersPaused:0 ContainersStopped:5 Images:25 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:190 OomKillDisable:true NGoroutines:195 SystemTime:2023-09-28 06:40:44.153406844 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:20 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:3962327040 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.21.0-desktop.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.7] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.24.1]] Warnings:<nil>}}
I0928 13:40:44.208192    6040 start_flags.go:305] no existing cluster config was found, will generate one from the flags 
I0928 13:40:44.277003    6040 start_flags.go:901] Wait components to verify : map[apiserver:true system_pods:true]
I0928 13:40:44.278108    6040 out.go:177] * Using Docker Desktop driver with root privileges
I0928 13:40:44.279190    6040 cni.go:84] Creating CNI manager for ""
I0928 13:40:44.279190    6040 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0928 13:40:44.279190    6040 start_flags.go:314] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0928 13:40:44.279190    6040 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:3000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\kyogi:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0928 13:40:44.280413    6040 out.go:177] * Starting control plane node minikube in cluster minikube
I0928 13:40:44.281460    6040 cache.go:122] Beginning downloading kic base image for docker with docker
I0928 13:40:44.282056    6040 out.go:177] * Pulling base image ...
I0928 13:40:44.283088    6040 preload.go:132] Checking if preload exists for k8s version v1.27.3 and runtime docker
I0928 13:40:44.283088    6040 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon
I0928 13:40:44.291209    6040 preload.go:148] Found local preload: C:\Users\kyogi\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.27.3-docker-overlay2-amd64.tar.lz4
I0928 13:40:44.291209    6040 cache.go:57] Caching tarball of preloaded images
I0928 13:40:44.291750    6040 preload.go:174] Found C:\Users\kyogi\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.27.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0928 13:40:44.292188    6040 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.3 on docker
I0928 13:40:44.293388    6040 profile.go:148] Saving config to C:\Users\kyogi\.minikube\profiles\minikube\config.json ...
I0928 13:40:44.293388    6040 lock.go:35] WriteFile acquiring C:\Users\kyogi\.minikube\profiles\minikube\config.json: {Name:mked5107c36a4f877492563fa01b0bda914bc60e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0928 13:40:44.549499    6040 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 to local cache
I0928 13:40:44.549499    6040 localpath.go:146] windows sanitize: C:\Users\kyogi\.minikube\cache\kic\amd64\kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631.tar -> C:\Users\kyogi\.minikube\cache\kic\amd64\kicbase_v0.0.40@sha256_8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631.tar
I0928 13:40:44.550006    6040 localpath.go:146] windows sanitize: C:\Users\kyogi\.minikube\cache\kic\amd64\kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631.tar -> C:\Users\kyogi\.minikube\cache\kic\amd64\kicbase_v0.0.40@sha256_8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631.tar
I0928 13:40:44.550006    6040 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local cache directory
I0928 13:40:44.552168    6040 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local cache directory, skipping pull
I0928 13:40:44.552168    6040 image.go:105] gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 exists in cache, skipping pull
I0928 13:40:44.552168    6040 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 as a tarball
I0928 13:40:44.552168    6040 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 from local cache
I0928 13:40:44.552755    6040 localpath.go:146] windows sanitize: C:\Users\kyogi\.minikube\cache\kic\amd64\kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631.tar -> C:\Users\kyogi\.minikube\cache\kic\amd64\kicbase_v0.0.40@sha256_8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631.tar
I0928 13:40:44.562313    6040 cache.go:169] failed to download gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631, will try fallback image if available: tarball: unexpected EOF
I0928 13:40:44.562313    6040 image.go:79] Checking for docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon
I0928 13:40:44.852556    6040 image.go:83] Found docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon, skipping pull
I0928 13:40:44.852556    6040 cache.go:145] docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 exists in daemon, skipping load
W0928 13:40:44.852556    6040 out.go:239] ! minikube was unable to download gcr.io/k8s-minikube/kicbase:v0.0.40, but successfully downloaded docker.io/kicbase/stable:v0.0.40 as a fallback image
I0928 13:40:44.852556    6040 cache.go:195] Successfully downloaded all kic artifacts
I0928 13:40:44.853172    6040 start.go:365] acquiring machines lock for minikube: {Name:mk4c075f799856a4189901b72f7ade1ac48c668d Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0928 13:40:44.853684    6040 start.go:369] acquired machines lock for "minikube" in 512.2Âµs
I0928 13:40:44.853684    6040 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:3000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\kyogi:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0} &{Name: IP: Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0928 13:40:44.853684    6040 start.go:125] createHost starting for "" (driver="docker")
I0928 13:40:44.854743    6040 out.go:204] * Creating docker container (CPUs=2, Memory=3000MB) ...
I0928 13:40:44.855384    6040 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0928 13:40:44.855384    6040 client.go:168] LocalClient.Create starting
I0928 13:40:44.856431    6040 main.go:141] libmachine: Reading certificate data from C:\Users\kyogi\.minikube\certs\ca.pem
I0928 13:40:44.869250    6040 main.go:141] libmachine: Decoding PEM data...
I0928 13:40:44.869250    6040 main.go:141] libmachine: Parsing certificate...
I0928 13:40:44.869760    6040 main.go:141] libmachine: Reading certificate data from C:\Users\kyogi\.minikube\certs\cert.pem
I0928 13:40:44.881184    6040 main.go:141] libmachine: Decoding PEM data...
I0928 13:40:44.881184    6040 main.go:141] libmachine: Parsing certificate...
I0928 13:40:44.905084    6040 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0928 13:40:45.159891    6040 network_create.go:76] Found existing network {name:minikube subnet:0xc000c90060 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 49 1] mtu:1500}
I0928 13:40:45.159891    6040 kic.go:117] calculated static IP "192.168.49.2" for the "minikube" container
I0928 13:40:45.197459    6040 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0928 13:40:45.521462    6040 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0928 13:40:45.797263    6040 oci.go:103] Successfully created a docker volume minikube
I0928 13:40:45.817839    6040 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 -d /var/lib
I0928 13:40:48.964197    6040 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 -d /var/lib: (3.146358s)
I0928 13:40:48.964197    6040 oci.go:107] Successfully prepared a docker volume minikube
I0928 13:40:48.964197    6040 preload.go:132] Checking if preload exists for k8s version v1.27.3 and runtime docker
I0928 13:40:48.964197    6040 kic.go:190] Starting extracting preloaded images to volume ...
I0928 13:40:48.979825    6040 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\kyogi\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.27.3-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 -I lz4 -xf /preloaded.tar -C /extractDir
I0928 13:41:53.460845    6040 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\kyogi\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.27.3-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 -I lz4 -xf /preloaded.tar -C /extractDir: (1m4.4810199s)
I0928 13:41:53.460845    6040 kic.go:199] duration metric: took 64.496648 seconds to extract preloaded images to volume
I0928 13:41:53.546889    6040 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0928 13:41:57.233802    6040 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (3.6869132s)
I0928 13:41:57.233802    6040 info.go:266] docker info: {ID:c6ecda50-44f3-4dd0-9dfa-13d2396a72bd Containers:42 ContainersRunning:37 ContainersPaused:0 ContainersStopped:5 Images:25 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:191 OomKillDisable:true NGoroutines:195 SystemTime:2023-09-28 06:41:57.114760095 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:20 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:3962327040 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.21.0-desktop.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.7] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.24.1]] Warnings:<nil>}}
I0928 13:41:57.257738    6040 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0928 13:41:59.866246    6040 cli_runner.go:217] Completed: docker info --format "'{{json .SecurityOptions}}'": (2.6085088s)
I0928 13:41:59.906232    6040 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3000mb --memory-swap=3000mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631
I0928 13:42:02.002243    6040 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3000mb --memory-swap=3000mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631: (2.0960117s)
I0928 13:42:02.032092    6040 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0928 13:42:02.693150    6040 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0928 13:42:03.213923    6040 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0928 13:42:04.195609    6040 oci.go:144] the created container "minikube" has a running status.
I0928 13:42:04.195609    6040 kic.go:221] Creating ssh key for kic: C:\Users\kyogi\.minikube\machines\minikube\id_rsa...
I0928 13:42:04.967610    6040 kic_runner.go:191] docker (temp): C:\Users\kyogi\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0928 13:42:06.703722    6040 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0928 13:42:07.482740    6040 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0928 13:42:07.482740    6040 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0928 13:42:09.044780    6040 kic_runner.go:123] Done: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]: (1.5620402s)
I0928 13:42:09.049673    6040 kic.go:261] ensuring only current user has permissions to key file located at : C:\Users\kyogi\.minikube\machines\minikube\id_rsa...
I0928 13:42:12.441111    6040 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0928 13:42:13.439143    6040 machine.go:88] provisioning docker machine ...
I0928 13:42:13.439143    6040 ubuntu.go:169] provisioning hostname "minikube"
I0928 13:42:13.517167    6040 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0928 13:42:14.644893    6040 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (1.1277268s)
I0928 13:42:14.663572    6040 main.go:141] libmachine: Using SSH client type: native
I0928 13:42:14.708682    6040 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xda8da0] 0xdabc40 <nil>  [] 0s} 127.0.0.1 61367 <nil> <nil>}
I0928 13:42:14.708682    6040 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0928 13:42:15.629714    6040 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0928 13:42:15.677557    6040 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0928 13:42:17.101030    6040 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (1.4234736s)
I0928 13:42:17.133847    6040 main.go:141] libmachine: Using SSH client type: native
I0928 13:42:17.133847    6040 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xda8da0] 0xdabc40 <nil>  [] 0s} 127.0.0.1 61367 <nil> <nil>}
I0928 13:42:17.133847    6040 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0928 13:42:18.190301    6040 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0928 13:42:18.190301    6040 ubuntu.go:175] set auth options {CertDir:C:\Users\kyogi\.minikube CaCertPath:C:\Users\kyogi\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\kyogi\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\kyogi\.minikube\machines\server.pem ServerKeyPath:C:\Users\kyogi\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\kyogi\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\kyogi\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\kyogi\.minikube}
I0928 13:42:18.190301    6040 ubuntu.go:177] setting up certificates
I0928 13:42:18.190301    6040 provision.go:83] configureAuth start
I0928 13:42:18.255674    6040 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0928 13:42:18.549901    6040 provision.go:138] copyHostCerts
I0928 13:42:18.550284    6040 exec_runner.go:144] found C:\Users\kyogi\.minikube/ca.pem, removing ...
I0928 13:42:18.550284    6040 exec_runner.go:203] rm: C:\Users\kyogi\.minikube\ca.pem
I0928 13:42:18.550793    6040 exec_runner.go:151] cp: C:\Users\kyogi\.minikube\certs\ca.pem --> C:\Users\kyogi\.minikube/ca.pem (1074 bytes)
I0928 13:42:18.553610    6040 exec_runner.go:144] found C:\Users\kyogi\.minikube/cert.pem, removing ...
I0928 13:42:18.553610    6040 exec_runner.go:203] rm: C:\Users\kyogi\.minikube\cert.pem
I0928 13:42:18.554119    6040 exec_runner.go:151] cp: C:\Users\kyogi\.minikube\certs\cert.pem --> C:\Users\kyogi\.minikube/cert.pem (1119 bytes)
I0928 13:42:18.572027    6040 exec_runner.go:144] found C:\Users\kyogi\.minikube/key.pem, removing ...
I0928 13:42:18.572027    6040 exec_runner.go:203] rm: C:\Users\kyogi\.minikube\key.pem
I0928 13:42:18.572768    6040 exec_runner.go:151] cp: C:\Users\kyogi\.minikube\certs\key.pem --> C:\Users\kyogi\.minikube/key.pem (1675 bytes)
I0928 13:42:18.574367    6040 provision.go:112] generating server cert: C:\Users\kyogi\.minikube\machines\server.pem ca-key=C:\Users\kyogi\.minikube\certs\ca.pem private-key=C:\Users\kyogi\.minikube\certs\ca-key.pem org=kyogi.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0928 13:42:18.847067    6040 provision.go:172] copyRemoteCerts
I0928 13:42:18.890331    6040 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0928 13:42:18.915944    6040 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0928 13:42:19.194211    6040 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61367 SSHKeyPath:C:\Users\kyogi\.minikube\machines\minikube\id_rsa Username:docker}
I0928 13:42:19.449184    6040 ssh_runner.go:362] scp C:\Users\kyogi\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0928 13:42:19.600469    6040 ssh_runner.go:362] scp C:\Users\kyogi\.minikube\machines\server.pem --> /etc/docker/server.pem (1196 bytes)
I0928 13:42:19.736148    6040 ssh_runner.go:362] scp C:\Users\kyogi\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0928 13:42:19.854096    6040 provision.go:86] duration metric: configureAuth took 1.6637943s
I0928 13:42:19.854096    6040 ubuntu.go:193] setting minikube options for container-runtime
I0928 13:42:19.857812    6040 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.3
I0928 13:42:19.889833    6040 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0928 13:42:20.381779    6040 main.go:141] libmachine: Using SSH client type: native
I0928 13:42:20.382941    6040 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xda8da0] 0xdabc40 <nil>  [] 0s} 127.0.0.1 61367 <nil> <nil>}
I0928 13:42:20.382941    6040 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0928 13:42:20.608102    6040 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0928 13:42:20.608102    6040 ubuntu.go:71] root file system type: overlay
I0928 13:42:20.608627    6040 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0928 13:42:20.630837    6040 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0928 13:42:21.341689    6040 main.go:141] libmachine: Using SSH client type: native
I0928 13:42:21.342803    6040 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xda8da0] 0xdabc40 <nil>  [] 0s} 127.0.0.1 61367 <nil> <nil>}
I0928 13:42:21.343313    6040 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0928 13:42:21.686658    6040 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0928 13:42:21.824668    6040 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0928 13:42:22.442970    6040 main.go:141] libmachine: Using SSH client type: native
I0928 13:42:22.444354    6040 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xda8da0] 0xdabc40 <nil>  [] 0s} 127.0.0.1 61367 <nil> <nil>}
I0928 13:42:22.444354    6040 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0928 13:42:25.070603    6040 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2023-07-07 14:50:55.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2023-09-28 06:42:21.703439912 +0000
@@ -1,30 +1,32 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +34,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0928 13:42:25.070603    6040 machine.go:91] provisioned docker machine in 11.6314608s
I0928 13:42:25.070603    6040 client.go:171] LocalClient.Create took 1m40.2152192s
I0928 13:42:25.070603    6040 start.go:167] duration metric: libmachine.API.Create for "minikube" took 1m40.2152192s
I0928 13:42:25.070603    6040 start.go:300] post-start starting for "minikube" (driver="docker")
I0928 13:42:25.070603    6040 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0928 13:42:25.106730    6040 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0928 13:42:25.130481    6040 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0928 13:42:25.491953    6040 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61367 SSHKeyPath:C:\Users\kyogi\.minikube\machines\minikube\id_rsa Username:docker}
I0928 13:42:25.695691    6040 ssh_runner.go:195] Run: cat /etc/os-release
I0928 13:42:25.706214    6040 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0928 13:42:25.706214    6040 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0928 13:42:25.706214    6040 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0928 13:42:25.706214    6040 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I0928 13:42:25.706905    6040 filesync.go:126] Scanning C:\Users\kyogi\.minikube\addons for local assets ...
I0928 13:42:25.707440    6040 filesync.go:126] Scanning C:\Users\kyogi\.minikube\files for local assets ...
I0928 13:42:25.707440    6040 start.go:303] post-start completed in 636.8367ms
I0928 13:42:25.734927    6040 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0928 13:42:25.958647    6040 profile.go:148] Saving config to C:\Users\kyogi\.minikube\profiles\minikube\config.json ...
I0928 13:42:26.009390    6040 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0928 13:42:26.028257    6040 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0928 13:42:26.300276    6040 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61367 SSHKeyPath:C:\Users\kyogi\.minikube\machines\minikube\id_rsa Username:docker}
I0928 13:42:26.505276    6040 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0928 13:42:26.523799    6040 start.go:128] duration metric: createHost completed in 1m41.6701153s
I0928 13:42:26.523799    6040 start.go:83] releasing machines lock for "minikube", held for 1m41.6701153s
I0928 13:42:26.551964    6040 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0928 13:42:26.864062    6040 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0928 13:42:26.906786    6040 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0928 13:42:26.923767    6040 ssh_runner.go:195] Run: cat /version.json
I0928 13:42:26.954313    6040 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0928 13:42:27.237382    6040 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61367 SSHKeyPath:C:\Users\kyogi\.minikube\machines\minikube\id_rsa Username:docker}
I0928 13:42:27.282937    6040 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61367 SSHKeyPath:C:\Users\kyogi\.minikube\machines\minikube\id_rsa Username:docker}
I0928 13:42:27.994959    6040 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.1308965s)
I0928 13:42:27.994959    6040 ssh_runner.go:235] Completed: cat /version.json: (1.0711919s)
I0928 13:42:28.063857    6040 ssh_runner.go:195] Run: systemctl --version
I0928 13:42:28.205906    6040 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0928 13:42:28.386731    6040 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0928 13:42:28.544415    6040 start.go:410] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0928 13:42:28.618293    6040 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0928 13:42:28.767722    6040 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0928 13:42:28.767722    6040 start.go:466] detecting cgroup driver to use...
I0928 13:42:28.768048    6040 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0928 13:42:28.768048    6040 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0928 13:42:28.924363    6040 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0928 13:42:29.034051    6040 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0928 13:42:29.067204    6040 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0928 13:42:29.107687    6040 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0928 13:42:29.202003    6040 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0928 13:42:29.274992    6040 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0928 13:42:29.428467    6040 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0928 13:42:29.589792    6040 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0928 13:42:29.777079    6040 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0928 13:42:29.898682    6040 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0928 13:42:30.018353    6040 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0928 13:42:30.115402    6040 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0928 13:42:30.451426    6040 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0928 13:42:30.683974    6040 start.go:466] detecting cgroup driver to use...
I0928 13:42:30.683974    6040 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0928 13:42:30.742294    6040 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0928 13:42:30.830030    6040 cruntime.go:276] skipping containerd shutdown because we are bound to it
I0928 13:42:30.906667    6040 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0928 13:42:30.952302    6040 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0928 13:42:31.174802    6040 ssh_runner.go:195] Run: which cri-dockerd
I0928 13:42:31.237477    6040 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0928 13:42:31.272334    6040 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0928 13:42:31.394738    6040 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0928 13:42:32.118394    6040 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0928 13:42:32.372569    6040 docker.go:535] configuring docker to use "cgroupfs" as cgroup driver...
I0928 13:42:32.372569    6040 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I0928 13:42:32.497956    6040 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0928 13:42:32.713954    6040 ssh_runner.go:195] Run: sudo systemctl restart docker
I0928 13:42:34.596452    6040 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.8824983s)
I0928 13:42:34.632401    6040 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0928 13:42:34.862970    6040 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0928 13:42:35.165478    6040 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0928 13:42:35.409068    6040 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0928 13:42:35.668625    6040 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0928 13:42:35.757096    6040 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0928 13:42:35.987044    6040 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0928 13:42:37.334211    6040 ssh_runner.go:235] Completed: sudo systemctl restart cri-docker: (1.3466481s)
I0928 13:42:37.334211    6040 start.go:513] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0928 13:42:37.376321    6040 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0928 13:42:37.388589    6040 start.go:534] Will wait 60s for crictl version
I0928 13:42:37.427192    6040 ssh_runner.go:195] Run: which crictl
I0928 13:42:37.474700    6040 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0928 13:42:38.907624    6040 ssh_runner.go:235] Completed: sudo /usr/bin/crictl version: (1.4329234s)
I0928 13:42:38.907624    6040 start.go:550] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1
I0928 13:42:38.932262    6040 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0928 13:42:39.631713    6040 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0928 13:42:40.599506    6040 out.go:204] * Preparing Kubernetes v1.27.3 on Docker 24.0.4 ...
I0928 13:42:40.619632    6040 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0928 13:42:41.651792    6040 cli_runner.go:217] Completed: docker exec -t minikube dig +short host.docker.internal: (1.03216s)
I0928 13:42:41.651792    6040 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0928 13:42:41.693844    6040 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0928 13:42:41.703038    6040 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0928 13:42:41.759042    6040 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0928 13:42:42.076047    6040 preload.go:132] Checking if preload exists for k8s version v1.27.3 and runtime docker
I0928 13:42:42.103867    6040 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0928 13:42:42.154414    6040 docker.go:636] Got preloaded images: -- stdout --
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
registry.k8s.io/kube-apiserver:v1.27.3
registry.k8s.io/kube-proxy:v1.27.3
registry.k8s.io/kube-controller-manager:v1.27.3
registry.k8s.io/kube-scheduler:v1.27.3
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0928 13:42:42.154414    6040 docker.go:566] Images already preloaded, skipping extraction
I0928 13:42:42.174884    6040 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0928 13:42:42.226643    6040 docker.go:636] Got preloaded images: -- stdout --
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
registry.k8s.io/kube-apiserver:v1.27.3
registry.k8s.io/kube-scheduler:v1.27.3
registry.k8s.io/kube-controller-manager:v1.27.3
registry.k8s.io/kube-proxy:v1.27.3
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0928 13:42:42.226643    6040 cache_images.go:84] Images are preloaded, skipping loading
I0928 13:42:42.244936    6040 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0928 13:42:43.767902    6040 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (1.5229652s)
I0928 13:42:43.768411    6040 cni.go:84] Creating CNI manager for ""
I0928 13:42:43.768411    6040 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0928 13:42:43.768411    6040 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0928 13:42:43.768411    6040 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.27.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0928 13:42:43.768411    6040 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0928 13:42:43.769100    6040 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0928 13:42:43.813319    6040 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.3
I0928 13:42:43.943214    6040 binaries.go:44] Found k8s binaries, skipping transfer
I0928 13:42:43.986480    6040 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0928 13:42:44.073184    6040 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0928 13:42:44.137524    6040 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0928 13:42:44.190336    6040 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0928 13:42:44.278717    6040 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0928 13:42:44.287242    6040 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0928 13:42:44.314918    6040 certs.go:56] Setting up C:\Users\kyogi\.minikube\profiles\minikube for IP: 192.168.49.2
I0928 13:42:44.315424    6040 certs.go:190] acquiring lock for shared ca certs: {Name:mk97fb21e5b979861901d9e46ba29ebbd6e36c9b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0928 13:42:44.395660    6040 certs.go:199] skipping minikubeCA CA generation: C:\Users\kyogi\.minikube\ca.key
I0928 13:42:44.413734    6040 certs.go:199] skipping proxyClientCA CA generation: C:\Users\kyogi\.minikube\proxy-client-ca.key
I0928 13:42:44.414802    6040 certs.go:319] generating minikube-user signed cert: C:\Users\kyogi\.minikube\profiles\minikube\client.key
I0928 13:42:44.414802    6040 crypto.go:68] Generating cert C:\Users\kyogi\.minikube\profiles\minikube\client.crt with IP's: []
I0928 13:42:44.863997    6040 crypto.go:156] Writing cert to C:\Users\kyogi\.minikube\profiles\minikube\client.crt ...
I0928 13:42:44.863997    6040 lock.go:35] WriteFile acquiring C:\Users\kyogi\.minikube\profiles\minikube\client.crt: {Name:mk47e8127376f6d31ed82e85c632c4048336122e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0928 13:42:44.863997    6040 crypto.go:164] Writing key to C:\Users\kyogi\.minikube\profiles\minikube\client.key ...
I0928 13:42:44.863997    6040 lock.go:35] WriteFile acquiring C:\Users\kyogi\.minikube\profiles\minikube\client.key: {Name:mkce9ac43030592c0cc4abc60ffd91e1d4497cf0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0928 13:42:44.863997    6040 certs.go:319] generating minikube signed cert: C:\Users\kyogi\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I0928 13:42:44.863997    6040 crypto.go:68] Generating cert C:\Users\kyogi\.minikube\profiles\minikube\apiserver.crt.dd3b5fb2 with IP's: [192.168.49.2 10.96.0.1 127.0.0.1 10.0.0.1]
I0928 13:42:45.383046    6040 crypto.go:156] Writing cert to C:\Users\kyogi\.minikube\profiles\minikube\apiserver.crt.dd3b5fb2 ...
I0928 13:42:45.383046    6040 lock.go:35] WriteFile acquiring C:\Users\kyogi\.minikube\profiles\minikube\apiserver.crt.dd3b5fb2: {Name:mkfcdbd44b21fd8f402f1109a85b8e4867318634 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0928 13:42:45.383046    6040 crypto.go:164] Writing key to C:\Users\kyogi\.minikube\profiles\minikube\apiserver.key.dd3b5fb2 ...
I0928 13:42:45.383046    6040 lock.go:35] WriteFile acquiring C:\Users\kyogi\.minikube\profiles\minikube\apiserver.key.dd3b5fb2: {Name:mkcb2e9c1051375b20f529aa54186a9f283a2bb5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0928 13:42:45.383046    6040 certs.go:337] copying C:\Users\kyogi\.minikube\profiles\minikube\apiserver.crt.dd3b5fb2 -> C:\Users\kyogi\.minikube\profiles\minikube\apiserver.crt
I0928 13:42:45.448506    6040 certs.go:341] copying C:\Users\kyogi\.minikube\profiles\minikube\apiserver.key.dd3b5fb2 -> C:\Users\kyogi\.minikube\profiles\minikube\apiserver.key
I0928 13:42:45.450089    6040 certs.go:319] generating aggregator signed cert: C:\Users\kyogi\.minikube\profiles\minikube\proxy-client.key
I0928 13:42:45.450089    6040 crypto.go:68] Generating cert C:\Users\kyogi\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I0928 13:42:45.857735    6040 crypto.go:156] Writing cert to C:\Users\kyogi\.minikube\profiles\minikube\proxy-client.crt ...
I0928 13:42:45.857735    6040 lock.go:35] WriteFile acquiring C:\Users\kyogi\.minikube\profiles\minikube\proxy-client.crt: {Name:mkaa92da9501b038e9ff5c61d55fc6e68915db4e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0928 13:42:45.857735    6040 crypto.go:164] Writing key to C:\Users\kyogi\.minikube\profiles\minikube\proxy-client.key ...
I0928 13:42:45.857735    6040 lock.go:35] WriteFile acquiring C:\Users\kyogi\.minikube\profiles\minikube\proxy-client.key: {Name:mk124bb7b56c86a47cefd0639845e700fae71236 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0928 13:42:45.882646    6040 certs.go:437] found cert: C:\Users\kyogi\.minikube\certs\C:\Users\kyogi\.minikube\certs\ca-key.pem (1679 bytes)
I0928 13:42:45.883154    6040 certs.go:437] found cert: C:\Users\kyogi\.minikube\certs\C:\Users\kyogi\.minikube\certs\ca.pem (1074 bytes)
I0928 13:42:45.883764    6040 certs.go:437] found cert: C:\Users\kyogi\.minikube\certs\C:\Users\kyogi\.minikube\certs\cert.pem (1119 bytes)
I0928 13:42:45.883764    6040 certs.go:437] found cert: C:\Users\kyogi\.minikube\certs\C:\Users\kyogi\.minikube\certs\key.pem (1675 bytes)
I0928 13:42:45.886187    6040 ssh_runner.go:362] scp C:\Users\kyogi\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0928 13:42:45.962434    6040 ssh_runner.go:362] scp C:\Users\kyogi\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0928 13:42:46.051927    6040 ssh_runner.go:362] scp C:\Users\kyogi\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0928 13:42:46.132281    6040 ssh_runner.go:362] scp C:\Users\kyogi\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0928 13:42:46.235144    6040 ssh_runner.go:362] scp C:\Users\kyogi\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0928 13:42:46.307101    6040 ssh_runner.go:362] scp C:\Users\kyogi\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0928 13:42:46.378725    6040 ssh_runner.go:362] scp C:\Users\kyogi\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0928 13:42:46.479909    6040 ssh_runner.go:362] scp C:\Users\kyogi\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0928 13:42:46.552318    6040 ssh_runner.go:362] scp C:\Users\kyogi\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0928 13:42:46.628882    6040 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0928 13:42:46.800199    6040 ssh_runner.go:195] Run: openssl version
I0928 13:42:46.952531    6040 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0928 13:42:47.027770    6040 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0928 13:42:47.036634    6040 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 May 20 04:01 /usr/share/ca-certificates/minikubeCA.pem
I0928 13:42:47.071061    6040 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0928 13:42:47.117178    6040 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0928 13:42:47.195018    6040 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0928 13:42:47.236996    6040 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0928 13:42:47.284856    6040 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0928 13:42:47.361802    6040 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0928 13:42:47.411057    6040 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0928 13:42:47.456644    6040 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0928 13:42:47.508283    6040 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0928 13:42:47.521924    6040 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:3000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\kyogi:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0928 13:42:47.539850    6040 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0928 13:42:47.736557    6040 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0928 13:42:47.771299    6040 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0928 13:42:47.771299    6040 kubeadm.go:636] restartCluster start
I0928 13:42:47.810225    6040 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0928 13:42:47.925546    6040 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0928 13:42:47.942166    6040 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0928 13:42:48.551712    6040 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in C:\Users\kyogi\.kube\config
I0928 13:42:48.552223    6040 kubeconfig.go:146] "minikube" context is missing from C:\Users\kyogi\.kube\config - will repair!
I0928 13:42:48.554031    6040 lock.go:35] WriteFile acquiring C:\Users\kyogi\.kube\config: {Name:mkf328761234a2ef81f42da06a55f68487449a2c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0928 13:42:48.932880    6040 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0928 13:42:48.967127    6040 kubeadm.go:602] needs reconfigure: configs differ:
-- stdout --
--- /var/tmp/minikube/kubeadm.yaml	2023-09-28 04:43:41.753350314 +0000
+++ /var/tmp/minikube/kubeadm.yaml.new	2023-09-28 06:42:44.263719292 +0000
@@ -38,7 +38,7 @@
     dataDir: /var/lib/minikube/etcd
     extraArgs:
       proxy-refresh-interval: "70000"
-kubernetesVersion: v1.27.4
+kubernetesVersion: v1.27.3
 networking:
   dnsDomain: cluster.local
   podSubnet: "10.244.0.0/16"

-- /stdout --
I0928 13:42:48.967127    6040 kubeadm.go:1128] stopping kube-system containers ...
I0928 13:42:48.992474    6040 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0928 13:42:49.079315    6040 docker.go:462] Stopping containers: [db2e9f299f03 4f8a9d130cf4 17b6ffe912dd ec06708054ee 03740da71a30 36d6f10ff64c adefaa6556ea 0a1896b1cdd2 3f0ce2dc0a83 3c47e95612a5 64dac2723388 627c18a58722 5cd29fcd0323 1512ee4bdb9b 07972984dbf9 be690e0ba05d f261d088b78d 5874174ff8f6 1490bdebecbc cbdbf512dd1f 5d59360b024c 04eff0ec64ef 1da63e65aae9 b301fe87efb8 34b5515c9a48 8cc1defe6570 a23941d7e1be]
I0928 13:42:49.114153    6040 ssh_runner.go:195] Run: docker stop db2e9f299f03 4f8a9d130cf4 17b6ffe912dd ec06708054ee 03740da71a30 36d6f10ff64c adefaa6556ea 0a1896b1cdd2 3f0ce2dc0a83 3c47e95612a5 64dac2723388 627c18a58722 5cd29fcd0323 1512ee4bdb9b 07972984dbf9 be690e0ba05d f261d088b78d 5874174ff8f6 1490bdebecbc cbdbf512dd1f 5d59360b024c 04eff0ec64ef 1da63e65aae9 b301fe87efb8 34b5515c9a48 8cc1defe6570 a23941d7e1be
I0928 13:42:49.229360    6040 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0928 13:42:49.699624    6040 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0928 13:42:49.807972    6040 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0928 13:42:49.839720    6040 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0928 13:42:49.866789    6040 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0928 13:42:49.866789    6040 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0928 13:42:52.252710    6040 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml": (2.3859208s)
E0928 13:42:52.252710    6040 kubeadm.go:717] sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml failed - will try once more: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml": Process exited with status 1
stdout:
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk

stderr:
error execution phase certs/apiserver-kubelet-client: [certs] certificate apiserver-kubelet-client not signed by CA certificate ca: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "minikubeCA")
To see the stack trace of this error execute with --v=5 or higher
I0928 13:42:52.253628    6040 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0928 13:42:52.378894    6040 kubeadm.go:640] restartCluster took 4.6075948s
W0928 13:42:52.378894    6040 out.go:239] ! Unable to restart cluster, will reset it: run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml": Process exited with status 1
stdout:
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk

stderr:
error execution phase certs/apiserver-kubelet-client: [certs] certificate apiserver-kubelet-client not signed by CA certificate ca: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "minikubeCA")
To see the stack trace of this error execute with --v=5 or higher

I0928 13:42:52.384629    6040 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force"
I0928 13:43:07.745987    6040 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force": (15.3613575s)
I0928 13:43:07.799026    6040 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0928 13:43:07.899203    6040 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0928 13:43:07.927918    6040 kubeadm.go:226] ignoring SystemVerification for kubeadm because of docker driver
I0928 13:43:07.962709    6040 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0928 13:43:07.988673    6040 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0928 13:43:07.989229    6040 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0928 13:43:08.125265    6040 kubeadm.go:322] [init] Using Kubernetes version: v1.27.3
I0928 13:43:08.126187    6040 kubeadm.go:322] [preflight] Running pre-flight checks
I0928 13:43:08.816482    6040 kubeadm.go:322] [preflight] Pulling images required for setting up a Kubernetes cluster
I0928 13:43:08.817578    6040 kubeadm.go:322] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0928 13:43:08.818086    6040 kubeadm.go:322] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0928 13:43:09.534096    6040 kubeadm.go:322] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0928 13:43:09.535354    6040 out.go:204]   - Generating certificates and keys ...
I0928 13:43:09.536382    6040 kubeadm.go:322] [certs] Using existing ca certificate authority
I0928 13:43:09.536382    6040 kubeadm.go:322] [certs] Using existing apiserver certificate and key on disk
I0928 13:43:09.546363    6040 kubeadm.go:322] 	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
I0928 13:43:09.547051    6040 kubeadm.go:322] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0928 13:43:09.547558    6040 kubeadm.go:322] 	[WARNING DirAvailable--var-lib-minikube-etcd]: /var/lib/minikube/etcd is not empty
I0928 13:43:09.548079    6040 kubeadm.go:322] error execution phase certs/apiserver-kubelet-client: [certs] certificate apiserver-kubelet-client not signed by CA certificate ca: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "minikubeCA")
I0928 13:43:09.548079    6040 kubeadm.go:322] To see the stack trace of this error execute with --v=5 or higher
W0928 13:43:09.548079    6040 out.go:239] ! initialization failed, will try again: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.27.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk

stderr:
	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
	[WARNING DirAvailable--var-lib-minikube-etcd]: /var/lib/minikube/etcd is not empty
error execution phase certs/apiserver-kubelet-client: [certs] certificate apiserver-kubelet-client not signed by CA certificate ca: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "minikubeCA")
To see the stack trace of this error execute with --v=5 or higher

I0928 13:43:09.549814    6040 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force"
I0928 13:43:16.176543    6040 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force": (6.6267286s)
I0928 13:43:16.223873    6040 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0928 13:43:16.257999    6040 kubeadm.go:226] ignoring SystemVerification for kubeadm because of docker driver
I0928 13:43:16.297107    6040 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0928 13:43:16.324651    6040 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0928 13:43:16.325162    6040 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0928 13:43:16.495583    6040 kubeadm.go:322] 	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
I0928 13:43:16.646435    6040 kubeadm.go:322] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0928 13:43:16.646833    6040 kubeadm.go:322] 	[WARNING DirAvailable--var-lib-minikube-etcd]: /var/lib/minikube/etcd is not empty
I0928 13:43:17.207125    6040 kubeadm.go:322] error execution phase certs/apiserver-kubelet-client: [certs] certificate apiserver-kubelet-client not signed by CA certificate ca: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "minikubeCA")
I0928 13:43:17.246570    6040 out.go:204]   - Generating certificates and keys ...
I0928 13:43:17.247077    6040 kubeadm.go:322] To see the stack trace of this error execute with --v=5 or higher
I0928 13:43:17.247077    6040 kubeadm.go:322] [init] Using Kubernetes version: v1.27.3
I0928 13:43:17.247077    6040 kubeadm.go:322] [preflight] Running pre-flight checks
I0928 13:43:17.247077    6040 kubeadm.go:322] [preflight] Pulling images required for setting up a Kubernetes cluster
I0928 13:43:17.247617    6040 kubeadm.go:322] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0928 13:43:17.247617    6040 kubeadm.go:322] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0928 13:43:17.247617    6040 kubeadm.go:322] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0928 13:43:17.248158    6040 kubeadm.go:322] [certs] Using existing ca certificate authority
I0928 13:43:17.248158    6040 kubeadm.go:322] [certs] Using existing apiserver certificate and key on disk
I0928 13:43:17.248158    6040 kubeadm.go:406] StartCluster complete in 29.7262338s
I0928 13:43:17.248158    6040 cri.go:54] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I0928 13:43:17.281727    6040 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-apiserver
I0928 13:43:17.414560    6040 cri.go:89] found id: ""
I0928 13:43:17.414560    6040 logs.go:284] 0 containers: []
W0928 13:43:17.414560    6040 logs.go:286] No container was found matching "kube-apiserver"
I0928 13:43:17.414560    6040 cri.go:54] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I0928 13:43:17.462051    6040 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=etcd
I0928 13:43:17.620022    6040 cri.go:89] found id: ""
I0928 13:43:17.620022    6040 logs.go:284] 0 containers: []
W0928 13:43:17.620022    6040 logs.go:286] No container was found matching "etcd"
I0928 13:43:17.620022    6040 cri.go:54] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I0928 13:43:17.688852    6040 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=coredns
I0928 13:43:17.844397    6040 cri.go:89] found id: "ec06708054ee0965027a521426706ecdf7a579582b11c9332525c9da17054db0"
I0928 13:43:17.844397    6040 cri.go:89] found id: "be690e0ba05d898b1ef53ffb058e6f2da12a1b45778311235707612798f75d8a"
I0928 13:43:17.844397    6040 cri.go:89] found id: ""
I0928 13:43:17.844397    6040 logs.go:284] 2 containers: [ec06708054ee0965027a521426706ecdf7a579582b11c9332525c9da17054db0 be690e0ba05d898b1ef53ffb058e6f2da12a1b45778311235707612798f75d8a]
I0928 13:43:17.884089    6040 ssh_runner.go:195] Run: which crictl
I0928 13:43:17.985291    6040 ssh_runner.go:195] Run: which crictl
I0928 13:43:17.995794    6040 cri.go:54] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I0928 13:43:18.042506    6040 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-scheduler
I0928 13:43:18.219352    6040 cri.go:89] found id: ""
I0928 13:43:18.219352    6040 logs.go:284] 0 containers: []
W0928 13:43:18.219352    6040 logs.go:286] No container was found matching "kube-scheduler"
I0928 13:43:18.219352    6040 cri.go:54] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I0928 13:43:18.290869    6040 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-proxy
I0928 13:43:18.397101    6040 cri.go:89] found id: ""
I0928 13:43:18.397101    6040 logs.go:284] 0 containers: []
W0928 13:43:18.397101    6040 logs.go:286] No container was found matching "kube-proxy"
I0928 13:43:18.397101    6040 cri.go:54] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I0928 13:43:18.429365    6040 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
I0928 13:43:18.534567    6040 cri.go:89] found id: ""
I0928 13:43:18.534567    6040 logs.go:284] 0 containers: []
W0928 13:43:18.534567    6040 logs.go:286] No container was found matching "kube-controller-manager"
I0928 13:43:18.534567    6040 cri.go:54] listing CRI containers in root : {State:all Name:kindnet Namespaces:[]}
I0928 13:43:18.576839    6040 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kindnet
I0928 13:43:18.696073    6040 cri.go:89] found id: ""
I0928 13:43:18.696073    6040 logs.go:284] 0 containers: []
W0928 13:43:18.696073    6040 logs.go:286] No container was found matching "kindnet"
I0928 13:43:18.696073    6040 logs.go:123] Gathering logs for coredns [be690e0ba05d898b1ef53ffb058e6f2da12a1b45778311235707612798f75d8a] ...
I0928 13:43:18.697098    6040 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 be690e0ba05d898b1ef53ffb058e6f2da12a1b45778311235707612798f75d8a"
I0928 13:43:18.830674    6040 logs.go:123] Gathering logs for Docker ...
I0928 13:43:18.830674    6040 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0928 13:43:18.963056    6040 logs.go:123] Gathering logs for container status ...
I0928 13:43:18.963056    6040 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0928 13:43:19.067389    6040 logs.go:123] Gathering logs for kubelet ...
I0928 13:43:19.067389    6040 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0928 13:43:19.117668    6040 logs.go:123] Gathering logs for dmesg ...
I0928 13:43:19.117668    6040 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0928 13:43:19.161088    6040 logs.go:123] Gathering logs for describe nodes ...
I0928 13:43:19.161088    6040 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0928 13:43:19.847205    6040 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0928 06:43:19.829432    6418 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0928 06:43:19.830592    6418 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0928 06:43:19.831562    6418 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0928 06:43:19.832602    6418 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0928 06:43:19.833872    6418 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0928 06:43:19.829432    6418 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0928 06:43:19.830592    6418 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0928 06:43:19.831562    6418 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0928 06:43:19.832602    6418 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0928 06:43:19.833872    6418 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0928 13:43:19.847205    6040 logs.go:123] Gathering logs for coredns [ec06708054ee0965027a521426706ecdf7a579582b11c9332525c9da17054db0] ...
I0928 13:43:19.847205    6040 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 ec06708054ee0965027a521426706ecdf7a579582b11c9332525c9da17054db0"
W0928 13:43:20.010087    6040 out.go:369] Error starting cluster: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.27.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk

stderr:
	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
	[WARNING DirAvailable--var-lib-minikube-etcd]: /var/lib/minikube/etcd is not empty
error execution phase certs/apiserver-kubelet-client: [certs] certificate apiserver-kubelet-client not signed by CA certificate ca: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "minikubeCA")
To see the stack trace of this error execute with --v=5 or higher
W0928 13:43:20.010087    6040 out.go:239] * 
W0928 13:43:20.011726    6040 out.go:239] X Error starting cluster: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.27.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk

stderr:
	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
	[WARNING DirAvailable--var-lib-minikube-etcd]: /var/lib/minikube/etcd is not empty
error execution phase certs/apiserver-kubelet-client: [certs] certificate apiserver-kubelet-client not signed by CA certificate ca: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "minikubeCA")
To see the stack trace of this error execute with --v=5 or higher

W0928 13:43:20.014160    6040 out.go:239] * 
W0928 13:43:20.017666    6040 out.go:239] â­ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ®
â                                                                                             â
â    * If the above advice does not help, please let us know:                                 â
â      https://github.com/kubernetes/minikube/issues/new/choose                               â
â                                                                                             â
â    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    â
â                                                                                             â
â°ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ¯
I0928 13:43:20.141290    6040 out.go:177] 
W0928 13:43:20.339229    6040 out.go:239] X Exiting due to GUEST_START: failed to start node: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.27.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk

stderr:
	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
	[WARNING DirAvailable--var-lib-minikube-etcd]: /var/lib/minikube/etcd is not empty
error execution phase certs/apiserver-kubelet-client: [certs] certificate apiserver-kubelet-client not signed by CA certificate ca: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "minikubeCA")
To see the stack trace of this error execute with --v=5 or higher

W0928 13:43:20.340326    6040 out.go:239] * 
W0928 13:43:20.342000    6040 out.go:239] â­ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ®
â                                                                                             â
â    * If the above advice does not help, please let us know:                                 â
â      https://github.com/kubernetes/minikube/issues/new/choose                               â
â                                                                                             â
â    * Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    â
â                                                                                             â
â°ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ¯
I0928 13:43:20.445987    6040 out.go:177] 

* 
* ==> Docker <==
* Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=error msg="CNI failed to delete loopback network: could not retrieve port mappings: invalid key: \"format\\\"\""
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=error msg="invalid key: \"format\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDformat\""
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"format\\\"\""
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=error msg="Failed to delete corrupt checkpoint for sandbox format\": invalid key: \"format\\\"\""
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"format\\\"\". Proceed without further sandbox information."
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=error msg="invalid key: \"format\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDformat\""
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"format\\\"\""
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=error msg="CNI failed to delete loopback network: could not retrieve port mappings: invalid key: \"format\\\"\""
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=error msg="invalid key: \"format\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDformat\""
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"format\\\"\""
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=error msg="Failed to delete corrupt checkpoint for sandbox endpoint=\"/var/run/cri-dockerd.sock\": invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\". Proceed without further sandbox information."
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=error msg="invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDendpoint=\"/var/run/cri-dockerd.sock\""
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=error msg="CNI failed to delete loopback network: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=error msg="invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDendpoint=\"/var/run/cri-dockerd.sock\""
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=error msg="Failed to delete corrupt checkpoint for sandbox endpoint=\"/var/run/cri-dockerd.sock\": invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\". Proceed without further sandbox information."
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=error msg="invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDendpoint=\"/var/run/cri-dockerd.sock\""
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=error msg="CNI failed to delete loopback network: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=error msg="invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDendpoint=\"/var/run/cri-dockerd.sock\""
Sep 28 06:43:14 minikube cri-dockerd[1502]: time="2023-09-28T06:43:14Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="Failed to delete corrupt checkpoint for sandbox endpoint=\"/var/run/cri-dockerd.sock\": invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\". Proceed without further sandbox information."
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDendpoint=\"/var/run/cri-dockerd.sock\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="CNI failed to delete loopback network: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDendpoint=\"/var/run/cri-dockerd.sock\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="Failed to delete corrupt checkpoint for sandbox endpoint=\"/var/run/cri-dockerd.sock\": invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\". Proceed without further sandbox information."
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDendpoint=\"/var/run/cri-dockerd.sock\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="CNI failed to delete loopback network: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDendpoint=\"/var/run/cri-dockerd.sock\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="Failed to delete corrupt checkpoint for sandbox endpoint=\"/var/run/cri-dockerd.sock\": invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\". Proceed without further sandbox information."
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDendpoint=\"/var/run/cri-dockerd.sock\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="CNI failed to delete loopback network: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\"Failed to delete corrupt checkpoint for sandboxpodSandboxIDendpoint=\"/var/run/cri-dockerd.sock\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="Error deleting network when building cni runtime conf: could not retrieve port mappings: invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="Failed to delete corrupt checkpoint for sandbox URL=\"unix:///var/run/cri-dockerd.sock\": invalid key: \"URL=\\\"unix:///var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="Failed to delete corrupt checkpoint for sandbox URL=\"unix:///var/run/cri-dockerd.sock\": invalid key: \"URL=\\\"unix:///var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="Failed to delete corrupt checkpoint for sandbox URL=\"unix:///var/run/cri-dockerd.sock\": invalid key: \"URL=\\\"unix:///var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="Failed to delete corrupt checkpoint for sandbox URL=\"unix:///var/run/cri-dockerd.sock\": invalid key: \"URL=\\\"unix:///var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="Failed to delete corrupt checkpoint for sandbox URL=\"unix:///var/run/cri-dockerd.sock\": invalid key: \"URL=\\\"unix:///var/run/cri-dockerd.sock\\\"\""
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="Error deleting pod kube-system/coredns-5d78c9869d-zhlk7 from network {docker 03740da71a30ff16b7e0922d6a2ebaab7313f2773057f2c6c3c7b1138649b224}::loopback:: plugin type=\"loopback\" failed (delete): missing network name"
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="Error deleting pod kube-system/coredns-5d78c9869d-zhlk7 from network {docker 03740da71a30ff16b7e0922d6a2ebaab7313f2773057f2c6c3c7b1138649b224}::loopback:: plugin type=\"loopback\" failed (delete): missing network name"
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="Error deleting pod kube-system/coredns-5d78c9869d-zhlk7 from network {docker 03740da71a30ff16b7e0922d6a2ebaab7313f2773057f2c6c3c7b1138649b224}::loopback:: plugin type=\"loopback\" failed (delete): missing network name"
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="Error deleting pod kube-system/coredns-5d78c9869d-zhlk7 from network {docker 03740da71a30ff16b7e0922d6a2ebaab7313f2773057f2c6c3c7b1138649b224}::loopback:: plugin type=\"loopback\" failed (delete): missing network name"
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="Error deleting pod kube-system/coredns-5d78c9869d-zhlk7 from network {docker 03740da71a30ff16b7e0922d6a2ebaab7313f2773057f2c6c3c7b1138649b224}::loopback:: plugin type=\"loopback\" failed (delete): missing network name"
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="Error deleting pod kube-system/coredns-5d78c9869d-zhlk7 from network {docker 1490bdebecbc37af65280e564265eae3c91d333089546cff40dcbd581af62b9d}::loopback:: plugin type=\"loopback\" failed (delete): missing network name"
Sep 28 06:43:15 minikube cri-dockerd[1502]: time="2023-09-28T06:43:15Z" level=error msg="Error deleting pod kube-system/coredns-5d78c9869d-zhlk7 from network {docker 1490bdebecbc37af65280e564265eae3c91d333089546cff40dcbd581af62b9d}::loopback:: plugin type=\"loopback\" failed (delete): missing network name"
Sep 28 06:43:16 minikube cri-dockerd[1502]: time="2023-09-28T06:43:16Z" level=error msg="Error deleting pod kube-system/coredns-5d78c9869d-zhlk7 from network {docker 1490bdebecbc37af65280e564265eae3c91d333089546cff40dcbd581af62b9d}::loopback:: plugin type=\"loopback\" failed (delete): missing network name"
Sep 28 06:43:16 minikube cri-dockerd[1502]: time="2023-09-28T06:43:16Z" level=error msg="Error deleting pod kube-system/coredns-5d78c9869d-zhlk7 from network {docker 1490bdebecbc37af65280e564265eae3c91d333089546cff40dcbd581af62b9d}::loopback:: plugin type=\"loopback\" failed (delete): missing network name"
Sep 28 06:43:16 minikube cri-dockerd[1502]: time="2023-09-28T06:43:16Z" level=error msg="Error deleting pod kube-system/coredns-5d78c9869d-zhlk7 from network {docker 1490bdebecbc37af65280e564265eae3c91d333089546cff40dcbd581af62b9d}::loopback:: plugin type=\"loopback\" failed (delete): missing network name"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                ATTEMPT             POD ID              POD
ec06708054ee0       ead0a4a53df89       2 hours ago         Exited              coredns             1                   03740da71a30f       coredns-5d78c9869d-zhlk7
be690e0ba05d8       ead0a4a53df89       5 days ago          Exited              coredns             0                   1490bdebecbc3       coredns-5d78c9869d-zhlk7

* 
* ==> coredns [be690e0ba05d] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:57905 - 23700 "HINFO IN 6403919197552988079.6990838562193370496. udp 57 false 512" NOERROR qr,rd,ra 112 0.088829752s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [ec06708054ee] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] 127.0.0.1:46670 - 63834 "HINFO IN 6143185787591914758.7551718708137084202. udp 57 false 512" - - 0 6.004542118s
[ERROR] plugin/errors: 2 6143185787591914758.7551718708137084202. HINFO: read udp 10.244.0.3:54560->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:53078 - 2900 "HINFO IN 6143185787591914758.7551718708137084202. udp 57 false 512" - - 0 6.007493933s
[ERROR] plugin/errors: 2 6143185787591914758.7551718708137084202. HINFO: read udp 10.244.0.3:41048->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:49454 - 56848 "HINFO IN 6143185787591914758.7551718708137084202. udp 57 false 512" - - 0 4.000758861s
[ERROR] plugin/errors: 2 6143185787591914758.7551718708137084202. HINFO: read udp 10.244.0.3:42684->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:53477 - 57433 "HINFO IN 6143185787591914758.7551718708137084202. udp 57 false 512" - - 0 2.00382624s
[ERROR] plugin/errors: 2 6143185787591914758.7551718708137084202. HINFO: read udp 10.244.0.3:46483->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:33646 - 60551 "HINFO IN 6143185787591914758.7551718708137084202. udp 57 false 512" NOERROR qr,rd,ra 112 0.081292486s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* 
* ==> dmesg <==
* [Sep28 03:51] MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.
[  +0.000000]  #2 #3
[  +0.010539] PCI: Fatal: No config space access function found
[  +0.019861] PCI: System does not support PCI
[  +0.266105] kvm: already loaded the other module
[  +2.047560] FS-Cache: Duplicate cookie detected
[  +0.000649] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.000971] FS-Cache: O-cookie d=000000003d3cb232{9P.session} n=00000000d770c402
[  +0.000720] FS-Cache: O-key=[10] '34323934393337353332'
[  +0.001296] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.002439] FS-Cache: N-cookie d=000000003d3cb232{9P.session} n=0000000065385441
[  +0.008213] FS-Cache: N-key=[10] '34323934393337353332'
[  +0.978185] 9pnet_virtio: no channels available for device drvfs
[  +0.001113] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.130743] 9pnet_virtio: no channels available for device drvfs
[  +0.000875] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.395445] 9pnet_virtio: no channels available for device drvfs
[  +0.000736] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.138311] 9pnet_virtio: no channels available for device drvfs
[  +0.000754] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +1.347861] 9pnet_virtio: no channels available for device drvfs
[  +0.003346] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.141059] 9pnet_virtio: no channels available for device drvfs
[  +0.000817] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.685078] WSL (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000006]  failed 2
[  +0.087334] misc dxg: dxgk: dxgglobal_acquire_channel_lock: Failed to acquire global channel lock
[  +0.375010] 9pnet_virtio: no channels available for device drvfs
[  +0.078808] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.100221] 9pnet_virtio: no channels available for device drvfs
[  +0.136416] 9pnet_virtio: no channels available for device drvfs
[  +0.118728] 9pnet_virtio: no channels available for device drvfs
[  +0.102219] misc dxg: dxgk: dxgglobal_acquire_channel_lock: Failed to acquire global channel lock
[  +0.076699] 9pnet_virtio: no channels available for device drvfs
[  +0.032691] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.100131] 9pnet_virtio: no channels available for device drvfs
[  +0.101539] 9pnet_virtio: no channels available for device drvfs
[  +0.137724] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Jakarta not found. Is the tzdata package installed?
[  +3.743807] 9pnet_virtio: no channels available for device drvfs
[  +0.018083] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.213365] 9pnet_virtio: no channels available for device drvfs
[  +0.052085] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.206053] WSL (2) ERROR: UtilCreateProcessAndWait:662: /bin/mount failed with 2
[  +0.016997] WSL (1) ERROR: UtilCreateProcessAndWait:684: /bin/mount failed with status 0xff00

[  +0.006927] WSL (1) ERROR: ConfigMountFsTab:2483: Processing fstab with mount -a failed.
[  +0.006236] WSL (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000006]  failed 2
[  +0.049041] 9pnet_virtio: no channels available for device drvfs
[  +0.001455] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.133757] 9pnet_virtio: no channels available for device drvfs
[  +0.001076] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.072490] misc dxg: dxgk: dxgglobal_acquire_channel_lock: Failed to acquire global channel lock
[  +0.308625] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Jakarta not found. Is the tzdata package installed?
[Sep28 04:41] hrtimer: interrupt took 27463339 ns

* 
* ==> kernel <==
*  06:43:41 up  2:52,  0 users,  load average: 2.22, 1.66, 1.15
Linux minikube 5.15.90.1-microsoft-standard-WSL2 #1 SMP Fri Jan 27 02:56:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.2 LTS"

* 
* ==> kubelet <==
* -- No entries --

